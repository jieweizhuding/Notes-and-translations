## 经验误差与过拟合
<b>在训练过程中 </b>
分类错误的样本数占总样本数的比率为错误率
$$
E=\frac{a}{m}
$$
正确率为
$$
acc=1-E
$$
我们把学习器的实际预测输出与样本的真实输出之间的差异称为“误差”(error),
学习器在训练集上的误差称为“训练误差”(training error)或“经验误差”(empirical error)
在新样本上的误差称为“泛化误差”(generalization error)

<b>
我们希望学习器的经验误差与泛化误差都较小，但这样的学习器表现一般都不好。
</b>

```
因为学习器存在过拟合与欠拟合的状态，当经验误差很小时，学习器会将训练样本的特征当作所有样本的特征，导致泛化误差增大。欠拟合是训练太少，无法总结出泛化的特征。

其中，欠拟合容易解决，可以增加样本量或训练次数。

而过拟合是无法解决的，因为机器学习面临的问题通常是NP 难甚至更难，
而有效的学习算法必然是在多项式时间内运行完成，若可彻底避免过拟合，
则通过经验误差最小化就能获最优解，这就意味着我们构造性地证明了 “P=NP” ；
因此，只 要相信“P != NP”，过拟合就不可避免。
```

<b>综上：经验误差与泛化误差都较小的学习器不是我们需要的，相比之下，我们更希望得到泛化误差较小的学习器，因此我们需要找到一个最好的模型，也就是“模型选择问题”。</b>

## 评估方法
要找到最好的模型，我们需要找到一个方法评估学习器。即如何量化泛化误差？
这里有两种思路：将部分样本用于训练，或将所有样本用于训练。
1. 将部分样本进行训练
因为我们要量化泛化误差，所以我们需要测试集，这部分测试集只能来自于样本集，故我们将样本集进行拆分。
因为只使用了部分样本集进行训练，必然会导致误差的增大。
    1. 留出法
    我们最简单的思路就是划分样本，采取分层抽样，即测试集中正例比例与训练集中正例比例相同，反例比例亦然。
    但是因为划分的方法不同，每次训练集不同，得到的学习器也不同，测试得到的泛化误差也不同，因此我们需要采取多种方式划分样本集，并对最终泛化误差结果取平均值。
    一般会选择2/3或4/5的数据作为训练集。
    2. 交叉验证法
    该方法是将样本集分成k份，其中1份作为测试集，剩下的作为训练集，可以重复k次这个过程，也就是能够得到k份学习器，每个学习器有都有对应的测试集，最后对其取平均值。这样的操作成为k折交叉验证。
    但是与留出法类似，不同的划分方法会导致不同的学习器，因此需要采用不同的划分方法重复p次，再取平均值，称为p次k折交叉验证。
    但如果样本集大小为m，采取m折交叉验证，则不同的划分对结果无影响，且每次训练只少一个数据，因此认为其结果较为准确。这个方法也称为留一法。然而这种方法的计算开销是难以忍受的，并且因为EFL原则，其并不一定优于其他方法。
2. 将所有样本进行训练
    1. 自助法
    给定m大小的样本集D，每次从中随机拷贝一个数据放入数据集D'中，直到D'的大小为m，则D'中的元素可能会有重复。D中的数据一直没有被选到的概率为:
    $$
    lim_{m\rightarrow +\infin}(1-\frac{1}{m})^m=\frac{1}{e}\approx 0.368
    $$
    即大约有36%的元素没有被选中，故我们使用D'作为训练集，D/D'作为测试集，故我们有m个数据用于训练，还有大约1/3的数据用于测试。
    自助法在数据集较小、难以有效划分训练/测试集时很有用；此外，自助法能从初始数据集中产生多个不同的训练集,这对集成学习等方法有很大的好处.
    然而，自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差.因此,在初始数据量足够时，留出法和交叉验证法更常用一些。

## 调参与最终模型
大多数大模型学习算法会有许多参数。因此，在进行模型评估与选择时不只需要选择合适的学习算法，也要对算法参数进行调节。
调参与算法选择类似之处在于：调参与算法选择都是对每个选择进行训练，得到模型，再进行评估，找到最好的选择，即参数。
不同之处在于：参数往往是连续的，而算法是离散的，因此我们会给定参数的取值区间与取值步长，类似二分法分割区间，从中选择离散的值。
因此，我们得到的参数往往不是最佳参数，而是对最佳参数的逼近值。所以，对参数的选择往往会最终模型性能有关键性影响。

此外，在使用m大小的数据集选择最终的模型后，我们需要用这个数据集对模型再训练一遍，保证其使用了所有个样本进行训练，此时这个模型才可以正式交付。

需要区分得到的最终模型后测试的数据与调优时使用的数据，我们可以将原始数据进行划分：
原始数据
├── 训练集（80%） → 用于模型训练和验证
│   ├── 子训练集（60%） → 更新模型参数
│   └── 验证集（20%） → 调参、选模型
└── 测试集（20%） → 最终性能评估（仅用一次！）

## 性能度量
刚刚介绍的是对学习器泛化能力评估的方法，如何具体度量模型的性能，仅仅靠泛化误差肯定是不够的，因此我们需要进一步找到其他的参数。
<b>
回归任务中最常用的是“均方误差”：
</b>

$$
E(f;D)=\frac{1}{m}\sum_{i=1}^{n}(f(x_i)-y_i)^2
$$
更一般的：
$$
E(f;D)=\int_{x \sim D}p(i)(f(x)-y)^2dx
$$
<b>这里我们主要讨论分类任务中常见的性能变量：</b>

1. 错误率与精度

2. 查准率、查全率与F1
3. ROC与AUC
4. 代价敏感错误率与代价曲线
