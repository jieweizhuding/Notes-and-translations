## 普通线性回归
<b> 普通线性回归是追求最小方差(bait)</b>
假设X为m* n的矩阵，参数矩阵为k* 1的矩阵
$$
\hat y=X\theta
$$
$$
loss(\theta_i)=\sum_i(\hat y-y)^2
$$
对参数求导，并令偏导为0,解得
$$
\theta=(X^TX)^{-1}X^Ty
$$

## (Ridge)岭回归(L2正则化)
<b>为了解决矩阵共线性问题，需要使用岭回归，会导致误差增大，方差减小，即减少了过拟合问题，换句话说，就是减小了模型的敏感性</b>
$$
loss(\theta_i)=\sum(X\theta-y)^2+\alpha\sum\theta^2
$$
对参数求导，得到
$$
\theta=(X^TX+\alpha I )^{-1}X^Ty
$$
可以调节\alpha保证参数可解。
同时增大\alpha，最佳参数会趋向于0,但不会为0.

## Lasso回归(L1正则化)
<b>与岭回归类似，可以消除</b>
$$
loss(\theta_i)=\sum(X\theta-y)^2+\alpha\sum|\theta|
$$
当\alpha增大时，最佳参数可以成为0，能够去除无用的特征。

## Lasso与Ridge的比较
对比总结表
    
    
    特性               L1正则化（Lasso）	    L2正则化（Ridge）
    解的形式   	稀疏（部分系数为零）	  稠密（系数接近零但不为零）
    特征选择	          显式剔除无关特征	          隐式降低无关特征权重
    抗共线性	     弱（可能随机选择其中一个特征）	  强（共线性特征权重均衡）
    计算复杂度	   较高（需迭代优化）	      低（有解析解）
    适用数据维度	    高维（p≫np≫n）	    中低维或共线性数据
优先L1的场景：

    明确需要特征选择或解释性。

    特征维度极高，且预计大部分无关。

优先L2的场景：

    特征间存在共线性或业务上均可能相关。

    需要稳定解且不关心特征剔除。

