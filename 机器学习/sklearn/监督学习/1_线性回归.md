## 普通线性回归
<b> 普通线性回归是追求最小方差(bait)</b>
假设X为m* n的矩阵，参数矩阵为k* 1的矩阵
$$
\hat y=X\theta
$$
$$
loss(\theta_i)=\sum_i(\hat y-y)^2
$$
对参数求导，并令偏导为0,解得
$$
\theta=(X^TX)^{-1}X^Ty
$$

## (Ridge)岭回归(L2正则化)
<b>为了解决矩阵共线性问题，需要使用岭回归，会导致误差增大，方差减小，即减少了过拟合问题，换句话说，就是减小了模型的敏感性</b>
$$
loss(\theta_i)=\sum(X\theta-y)^2+\frac{\alpha}{2}\sum\theta^2
$$
对参数求导，得到
$$
\theta=(X^TX+\alpha I )^{-1}X^Ty
$$
可以调节\alpha保证参数可解。
同时增大\alpha，最佳参数会趋向于0,但不会为0.

## Lasso回归(L1正则化)
<b>与岭回归类似，可以消除</b>
$$
loss(\theta_i)=\sum(X\theta-y)^2+\alpha\sum|\theta|
$$
当\alpha增大时，最佳参数可以成为0，能够去除无用的特征。

### Lasso的求解
因为Lasso无法通过代数求解，故需要迭代，主要的方法有两种：
坐标轴下降法与最小角回归
坐标轴下降法：
选取某一个特征属性进行更新，并固定其余的属性，直到变化的属性达到最优值，再调整其他的属性
最小角回归
[最小角回归解释-CSDN](https://blog.csdn.net/guofei_fly/article/details/103845342)
用不严谨但易于理解的方式讲：最小角回归就是先找到最相关的向量，在这个方向一点点增大对应的系数，直到其与第二相关的向量相关性相同，(可以视为对角线方向)，并沿着对角线方向前进，直到这个方向与第三相关的向量相关性相同，并一直重复该做法，直到找到所有向量的对角线方向，并沿着这个方向走到残差为0即可

用最小角回归法什么时候比坐标轴下降法好呢？场景一：如果我们想探索超参数α更多的相关值的话，由于最小角回归可以看到回归路径，此时用LassoLarsCV比较好。场景二： 如果我们的样本数远小于样本特征数的话，用LassoLarsCV也比LassoCV好。其余场景最好用LassoCV。

## Lasso与Ridge的比较
对比总结表
    
    
    特性               L1正则化（Lasso）	    L2正则化（Ridge）
    解的形式   	稀疏（部分系数为零）	  稠密（系数接近零但不为零）
    特征选择	          显式剔除无关特征	          隐式降低无关特征权重
    抗共线性	     弱（可能随机选择其中一个特征）	  强（共线性特征权重均衡）
    计算复杂度	   较高（需迭代优化）	      低（有解析解）
    适用数据维度	    高维（p≫np≫n）	    中低维或共线性数据
优先L1的场景：

    明确需要特征选择或解释性。

    特征维度极高，且预计大部分无关。

优先L2的场景：

    特征间存在共线性或业务上均可能相关。

    需要稳定解且不关心特征剔除。

## 弹性网络回归
该回归结合了L1与L2正则化，loss函数为
$$
loss=MSE+\lambda\alpha\sum|\theta|+\lambda\frac{(1-\alpha)}{2}\theta^2
$$
使用场景：
ElasticNetCV类用在我们发现用Lasso回归太过（太多特征被稀疏为0），而用Ridge回归又正则化的不够（回归系数衰减的太慢）的时候。一般不推荐拿到数据就直接就上ElasticNetCV。

## 正交追踪匹配(OMP)
与lasso类似，一般更倾向于使用lasso，OMP的优点是可以指明参数最终不为0的个数，因此在一些性能受限的场景与单片机，只能接受k个特征时，OMP更好。

## 贝叶斯回归
我们上面提到的都是频率派的方法，即设计出loss function，再一步步优化，借助MLE(极大似然估计)求解出参数，即优化问题。而贝叶斯派核心是求解积分，通过MAP(最大后验概率估计)求解出参数。
具体了解可以看：
[【机器学习 概率派与贝叶斯派】](https://www.bilibili.com/video/BV1aE411o7qd/?p=2&share_source=copy_web&vd_source=897677428a7edfa0638cc3b6e1a1b405)
## TODOS: 后面补贝叶斯分布的内容
如果我们的数据有很多缺失或者矛盾的病态数据，可以考虑BayesianRidge类，它对病态数据鲁棒性很高，也不用交叉验证选择超参数。但是极大化似然函数的推断过程比较耗时，一般情况不推荐使用。


总结一下使用场景：
1. SDG回归(当数据量大于10,000时一般选择SDG回归)
2. ridge与lasso回归(分别具有不同的特点，看更注重那一方面进行选择)
3. ElasticNet回归(在没有特殊要求时，且ridge与lasso效果不佳时可以考虑尝试一下)
4. OMP回归(对特征数量有明确要求)
5. 贝叶斯回归(数据有大量缺失或病态数据)